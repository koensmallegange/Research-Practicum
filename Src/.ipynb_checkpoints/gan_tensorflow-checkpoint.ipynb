{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1365ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ac46814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give all the plots a seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Give all the plots LaTex font and text    \n",
    "rc('text', usetex=True)\n",
    "rc('font', family='serif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f436e6c2",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3d4ebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "Gold = pd.read_excel(r\"C:\\Users\\koens\\OneDrive\\Bureaublad\\Research-Practicum\\Data\\Real Data\\FUT_Option.xlsx\")\n",
    "\n",
    "# Convert to datetime\n",
    "Gold['date'] = pd.to_datetime(Gold['date'])\n",
    "Gold['futures_expiration_date'] = pd.to_datetime(Gold['futures_expiration_date'])\n",
    "Gold['options_expiration_date'] = pd.to_datetime(Gold['options_expiration_date'], errors='coerce')\n",
    "\n",
    "Gold = Gold[Gold['options_expiration_date'] >= '2019-10-18']\n",
    "Gold['TTM'] = (Gold['options_expiration_date'] - Gold['date']).dt.days / 365.25\n",
    "\n",
    "# Rescale\n",
    "Gold['futures_close'] = Gold['futures_close']/1000000\n",
    "Gold['strike'] = Gold['strike']/1000000\n",
    "Gold['bid'] = Gold['bid']/1000000\n",
    "Gold['ask'] = Gold['ask']/1000000\n",
    "Gold['settlement'] = Gold['settlement']/1000000\n",
    "Gold['vega'] = Gold['vega']/1000000\n",
    "\n",
    "# Isolate call and put\n",
    "Gold_call = Gold[Gold['call_put'] == 'C'].copy()\n",
    "Gold_put = Gold[Gold['call_put'] == 'P'].copy()\n",
    "\n",
    "# Sort by date\n",
    "Gold_call.sort_values('date', inplace=True)\n",
    "Gold_put.sort_values('date', inplace=True)\n",
    "\n",
    "# Drop non-numeric columns if there are any\n",
    "data = Gold_call.select_dtypes(include=[np.number])\n",
    "data = data.drop(['delta', 'vega', 'gamma', 'theta'], axis=1)\n",
    "\n",
    "# Filter out rows with IV > 1000\n",
    "mask = data['iv'] > 1000\n",
    "data = data[~mask]\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "data_scaled = scaler.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48814d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0546bd84",
   "metadata": {},
   "source": [
    "# Build the Market Data Simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1077a6",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dec88fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "NOISE_DIM = 75\n",
    "BUFFER_SIZE = 5389 \n",
    "EPOCHS = 7500\n",
    "random_samples = 5211\n",
    "\n",
    "# Define the training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(data_scaled).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "# Overige params\n",
    "accuracy = 5\n",
    "step_daily = 0.00273785078\n",
    "step_weekly = 0.14285714285\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b514ec",
   "metadata": {},
   "source": [
    "### Define the GAN's networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd4c15f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_generator_model():\n",
    "    '''Generates a network that generates synthetic data'''\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(BATCH_SIZE, activation='relu', input_shape=(NOISE_DIM,)),\n",
    "        layers.Dense(data_scaled.shape[1], activation='tanh')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def make_discriminator_model():\n",
    "    '''Generates a network that discriminates real from synthetic data'''\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(BATCH_SIZE, activation='relu', input_shape=(data_scaled.shape[1],)),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22124597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\koens\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the models\n",
    "generator = make_generator_model()\n",
    "discriminator = make_discriminator_model()\n",
    "\n",
    "# Define the loss and optimizers\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
    "generator_optimizer = tf.keras.optimizers.Adam()\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6304ac8f",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d703ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Epoch 2806/7500 completed'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the training loop\n",
    "@tf.function\n",
    "def train_step(real_data):\n",
    "    # Sample noise from a normal distribution\n",
    "    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n",
    "\n",
    "    # Train the discriminator\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_data = generator(noise, training=True)\n",
    "        \n",
    "        # Discriminate real from fake data\n",
    "        real_output = discriminator(real_data, training=True)\n",
    "        fake_output = discriminator(generated_data, training=True)\n",
    "        \n",
    "        # Calculate the losses\n",
    "        gen_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "        disc_loss = (cross_entropy(tf.ones_like(real_output), real_output) +\n",
    "                     cross_entropy(tf.zeros_like(fake_output), fake_output))\n",
    "        \n",
    "    # Calculate the gradients\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    \n",
    "    # Update the weights\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "# Training loop\n",
    "def train(dataset, epochs):\n",
    "    '''Trains the GAN'''\n",
    "    for epoch in range(epochs):\n",
    "        for data_batch in dataset:\n",
    "            train_step(data_batch)\n",
    "\n",
    "        # Display progress\n",
    "        clear_output(wait=True)\n",
    "        display(f'Epoch {epoch + 1}/{epochs} completed')\n",
    "        \n",
    "# Initialize the training\n",
    "train(train_dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327aed90-54f4-435b-a2d8-bf8cfa6ed7fc",
   "metadata": {},
   "source": [
    "### Get the synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b77b52e-b19b-4ff5-be1a-95ed39924d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random noise\n",
    "random_noise = tf.random.normal([random_samples, NOISE_DIM])\n",
    "\n",
    "# Use the generator to create option prices\n",
    "simulated_data = generator(random_noise, training=False)\n",
    "\n",
    "# Rescale the data\n",
    "simulated_data_rescaled = scaler.inverse_transform(simulated_data)\n",
    "\n",
    "# Convert the generated data to pandas df\n",
    "simulated_data_df = pd.DataFrame(simulated_data_rescaled, columns=data.columns)\n",
    "\n",
    "# Save synthetic data to drive\n",
    "simulated_data_df.to_excel(\"C:\\Users\\koens\\OneDrive\\Bureaublad\\Research-Practicum\\Data\\Synthetic Data\\synthetic_gold_lowsamples_7500.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054eca3f",
   "metadata": {},
   "source": [
    "### Hyperparameters meaning\n",
    "\n",
    "BATCH_SIZE (256): Number of training examples used in one iteration of model training. In a dataset, data is usually divided into batches, and each batch is fed into the network one at a time. A batch size of 256 means that 256 data points from the dataset are used for each training step.\n",
    "\n",
    "NOISE_DIM (100): Refers to the dimensionality of the random noise vector that is input into the generator network. A noise dimension of 100 means that the generator takes in a random vector of size 100 to generate data.\n",
    "\n",
    "BUFFER_SIZE (60000): Defines the size of the buffer used to shuffle the dataset. A larger buffer size ensures better randomization of data. In TensorFlow, for example, Dataset.shuffle(buffer_size) randomly shuffles the elements of the dataset. A buffer size of 60000 in this context would mean the dataset is shuffled using a buffer that can hold 60000 elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160c3427",
   "metadata": {},
   "source": [
    "# Analyze the Synthethic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0193b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_data_df.tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d959bd",
   "metadata": {},
   "source": [
    "### Discretize the Time to Maturity (TTM)\n",
    "\n",
    "Replaces each TTM value in the generated data with the nearest TTM value from the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac90dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# Quantize TTM values to the nearest multiple of the step\n",
    "# simulated_data_df['TTM'] = (np.round(simulated_data_df['TTM'] / step_daily) * step_daily).astype(float)\n",
    "\n",
    "# Convert the original TTM values into a KDTree for efficient nearest neighbor search\n",
    "# original_TTM_tree = cKDTree(np.array(data['TTM']).reshape(-1, 1))\n",
    "\n",
    "# Replace generated TTM values with nearest original TTM values\n",
    "# generated_TTM_values = simulated_data_df['TTM']\n",
    "# nearest_indices = original_TTM_tree.query(generated_TTM_values.values.reshape(-1, 1))[1]\n",
    "# simulated_data_df['TTM'] = np.array(data['TTM'])[nearest_indices]\n",
    "\n",
    "# Convert the original future_close values into a KDTree for efficient nearest neighbor search\n",
    "# original_TTM_tree = cKDTree(np.array(data['futures_close']).reshape(-1, 1))\n",
    "\n",
    "# Replace generated future_close values with nearest original future_close values\n",
    "# generated_TTM_values = simulated_data_df['futures_close']\n",
    "# nearest_indices = original_TTM_tree.query(generated_TTM_values.values.reshape(-1, 1))[1]\n",
    "# simulated_data_df['futures_close'] = np.array(data['futures_close'])[nearest_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f11453c",
   "metadata": {},
   "source": [
    "### Round the prices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b807112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_data_df['strike'] = (simulated_data_df['strike'] / accuracy).round() * accuracy\n",
    "simulated_data_df['Moneyness'] = simulated_data_df['strike']/simulated_data_df['futures_close']\n",
    "\n",
    "# print amount of rows in simulated data\n",
    "print('Amount of rows in simulated data: ' + str(len(simulated_data_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20d3223",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b31f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_data_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc01288",
   "metadata": {},
   "source": [
    "### Plot features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474017d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data['iv'], data['TTM'], alpha = 0.75, s=5, label = \"Real\")\n",
    "plt.scatter(simulated_data_df['iv'], simulated_data_df['TTM'], alpha = 0.75, s=5, label = \"GAN\")\n",
    "plt.title(\"Volatility distribution of real and synthetic data\")\n",
    "plt.xlabel(\"Implied volatility\")\n",
    "plt.ylabel(\"TTM\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf37c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d522710",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_data_df['TTM'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063ca52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ttms = simulated_data_df['TTM'].unique()\n",
    "target_ttms = np.arange(0, max(unique_ttms).round(), 0.5)\n",
    "closest_ttms = np.array([unique_ttms[np.argmin(np.abs(unique_ttms - target))] for target in target_ttms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9887e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot 'strike' against 'iv' for each 'TTM'\n",
    "for ttm in unique_ttms:\n",
    "    sorted_data = simulated_data_df.sort_values(by='Moneyness')\n",
    "    subset = simulated_data_df[simulated_data_df['TTM'] == ttm]\n",
    "    plt.figure()\n",
    "    plt.plot(sorted_data['Moneyness'], sorted_data['iv'], alpha = 0.75)\n",
    "    plt.title(f'TTM = {ttm.round}')\n",
    "    plt.xlabel('Moneyness')\n",
    "    plt.ylabel('IV')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2622df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot 'IV' against 'Moneyness' for each 'TTM'\n",
    "for ttm in unique_ttms:\n",
    "    # Create a subset of the data for each TTM\n",
    "    subset = simulated_data_df[simulated_data_df['TTM'] == ttm]\n",
    "    subset_sorted = subset.sort_values(by='Moneyness')  \n",
    "    \n",
    "    # Plot the data\n",
    "    plt.figure()\n",
    "    plt.plot(subset_sorted['Moneyness'], subset_sorted['iv'], alpha = 0.75) \n",
    "    plt.title(f'TTM = {ttm.round(4)}')\n",
    "    plt.xlabel('Moneyness')\n",
    "    plt.ylabel('IV')\n",
    "    plt.grid(True) \n",
    "    plt.show()  \n",
    "\n",
    "# Remove plt.tight_layout() as it is not needed when showing plots one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18287553",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_data_df['futures_close'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb5b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)  \n",
    "plt.hist(simulated_data_df['strike'], bins=30, alpha=0.7)\n",
    "plt.title('GAN')\n",
    "plt.xlabel('Strike Price')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)  \n",
    "plt.hist(data['strike'], bins=30, alpha=0.7)\n",
    "plt.title('Real')\n",
    "plt.xlabel('Strike Price')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "fig.suptitle('Strike Price Distribution of Real and Synthetic data', fontsize=16)\n",
    "plt.tight_layout()  \n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)  \n",
    "plt.hist(simulated_data_df['TTM'], bins=60, alpha=0.7)\n",
    "plt.title('GAN')\n",
    "plt.xlabel('TTM')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)  \n",
    "plt.hist(data['TTM'], bins=60, alpha=0.7)\n",
    "plt.title('Real')\n",
    "plt.xlabel('TTM')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "fig.suptitle('Time to Maturity Distribution of Real and Synthetic Data', fontsize=16)\n",
    "plt.tight_layout()  \n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)  \n",
    "plt.hist(simulated_data_df['iv'], bins=30, alpha=0.7)\n",
    "plt.title('GAN')\n",
    "plt.xlabel('Implied Volatility')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)  \n",
    "plt.hist(data['iv'], bins=30, alpha=0.7)\n",
    "plt.title('Real')\n",
    "plt.xlabel('Implied Volatility')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "fig.suptitle('Implied Volatility Distribution of Synthetic and Real Option Data', fontsize=16)\n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eec8bd9",
   "metadata": {},
   "source": [
    "unique_combinations = simulated_data_df[['TTM', 'futures_close']].drop_duplicates()\n",
    "kurtosis = []\n",
    "skewness = []\n",
    "labels = []\n",
    "for index, row in unique_combinations.iterrows():\n",
    "    specific_expiration_date = row['TTM']\n",
    "    specific_future_close = row['futures_close']\n",
    "    \n",
    "    # Filter data for the specific combination\n",
    "    specific_conditions = (simulated_data_df['TTM'] == specific_expiration_date) & (simulated_data_df['futures_close'] == specific_future_close)\n",
    "    filtered_data = simulated_data_df[specific_conditions]\n",
    "    \n",
    "    sorted_data = filtered_data.sort_values(by='Moneyness')\n",
    "    kurtosis_value = filtered_data['iv'].kurtosis()\n",
    "    if kurtosis_value <20:\n",
    "        kurtosis.append(kurtosis_value)\n",
    "        labels.append(f'{specific_expiration_date} - {specific_future_close}')\n",
    "        skewness.append(filtered_data['iv'].skew())\n",
    "    plt.figure()\n",
    "    plt.plot(sorted_data['Moneyness'], sorted_data['iv'])\n",
    "    plt.title(f'Implied Volatility vs Moneyness\\nExpiration Date: {specific_expiration_date}, Future Close: {specific_future_close}')\n",
    "    plt.xlabel('Moneyness')\n",
    "    plt.ylabel('Implied Volatility')\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdafaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a polynomial of degree n (e.g., n=3 for cubic)\n",
    "n = 3\n",
    "\n",
    "for ttm in unique_ttms:\n",
    "    subset = simulated_data_df[simulated_data_df['TTM'] == ttm]\n",
    "    subset_sorted = subset.sort_values(by='Moneyness')  \n",
    "    coefficients = np.polyfit(subset_sorted['Moneyness'], subset_sorted['iv'], n)\n",
    "\n",
    "    # Create a polynomial function with the obtained coefficients\n",
    "    polynomial = np.poly1d(coefficients)\n",
    "\n",
    "    # Generate x values for plotting the polynomial curve\n",
    "    x_values = np.linspace(subset_sorted['Moneyness'].min(), subset_sorted['Moneyness'].max(), 100)\n",
    "\n",
    "    # Plot the original data and the fitted polynomial curve\n",
    "    plt.figure()\n",
    "    plt.scatter(subset_sorted['Moneyness'], subset_sorted['iv'], label='Original Data', s = 5)\n",
    "    plt.plot(x_values, polynomial(x_values), color='red', label=f'{n}-degree Polynomial Fit')\n",
    "    plt.title(f'Volatility Smile with Polynomial Fit of TTM = {ttm}')\n",
    "    plt.xlabel('Moneyness')\n",
    "    plt.ylabel('IV')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aff997f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab34e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decd492d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a47f0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc254b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccef9b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1e0ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a5d5f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b7c294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
